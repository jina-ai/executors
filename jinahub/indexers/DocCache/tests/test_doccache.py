import os
import shutil
from pathlib import Path

import pytest
from jina import DocumentArray, Document, Executor, Flow

from ..doc_cache import DocCache

cur_dir = os.path.dirname(os.path.abspath(__file__))
default_config = os.path.abspath(os.path.join(cur_dir, '..', 'config.yml'))


def test_config():
    ex = Executor.load_config(str(Path(__file__).parents[2] / 'config.yml'))


@pytest.mark.parametrize('cache_fields', ['[content_hash]', '[id]'])
def test_cache(tmpdir, cache_fields):
    os.environ['CACHE_FIELDS'] = cache_fields
    os.environ['CACHE_WORKSPACE'] = os.path.join(tmpdir, 'cache')
    docs = []
    docs2 = []

    if cache_fields == '[content_hash]':
        docs = [Document(content='a'), Document(content='a')]
        docs2 = [Document(content='b'), Document(content='a')]
    elif cache_fields == '[id]':
        docs = [Document(id='a'), Document(id='a')]
        docs2 = [Document(id='b'), Document(id='a')]

    with Flow().add(uses=os.path.join(cur_dir, 'cache.yml')) as f:
        response = f.post(on='/index', inputs=DocumentArray(docs), return_results=True)
        assert len(response[0].docs) == 1
        if cache_fields == '[content_hash]':
            assert set([d.content for d in response[0].docs]) == {'a'}
        elif cache_fields == '[id]':
            assert set([d.id for d in response[0].docs]) == {'a'}

        response = f.post(on='/index', inputs=DocumentArray(docs2), return_results=True)
        assert len(response[0].docs) == 1
        # assert the correct docs have been removed
        if cache_fields == '[content_hash]':
            assert set([d.content for d in response[0].docs]) == {'b'}
        elif cache_fields == '[id]':
            assert set([d.id for d in response[0].docs]) == {'b'}


def test_cache_id_content_hash(tmpdir):
    os.environ['CACHE_FIELDS'] = '[id, content]'
    os.environ['CACHE_WORKSPACE'] = os.path.join(tmpdir, 'cache')
    docs = [
        Document(id='a', content='content'),
        Document(id='a', content='content'),
        Document(id='a', content='content'),
    ]
    with Flow(return_results=True).add(uses=os.path.join(cur_dir, 'cache.yml')) as f:
        response = f.post(on='/index', inputs=DocumentArray(docs), return_results=True)
        assert len(response[0].docs) == 1
        # assert the correct docs have been removed
        assert set([d.content for d in response[0].docs]) == {'content'}
        assert set([d.id for d in response[0].docs]) == {'a'}


def test_cache_id_content_hash2(tmpdir):
    os.environ['CACHE_FIELDS'] = '[id, content_hash]'
    os.environ['CACHE_WORKSPACE'] = os.path.join(tmpdir, 'cache')
    docs2 = [
        Document(id='b', content='content'),
        Document(id='a', content='content'),
        Document(id='a', content='content'),
    ]
    with Flow(return_results=True).add(uses=os.path.join(cur_dir, 'cache.yml')) as f:
        response = f.post(on='/index', inputs=DocumentArray(docs2), return_results=True)
        assert len(response[0].docs) == 2


def test_cache_crud(tmpdir):
    docs = DocumentArray(
        [
            Document(id=1, content='content'),
            Document(id=2, content='content'),
            Document(id=3, content='content'),
            Document(id=4, content='content2'),
        ]
    )

    cache = DocCache(
        fields=('content_hash',),
        metas={'workspace': os.path.join(tmpdir, 'cache'), 'name': 'cache'},
        # runtime_args={'pea_id': 0},
    )
    cache.index_or_remove_from_request(docs)
    # we cache all the docs by id, we just remove the ones that have already been "hit"
    assert cache.ids_count == 4
    assert cache.hashes_count == 2

    docs = DocumentArray(
        [
            Document(id=1, content='content3'),
            Document(id=2, content='content4'),
            Document(id=3, content='contentX'),
            Document(id=4, content='contentBLA'),
        ]
    )

    cache.update(docs)
    assert cache.ids_count == 4
    assert cache.hashes_count == 4

    docs = DocumentArray(
        [
            Document(id=1),
            Document(id=2),
            Document(id=3),
            Document(id=4),
            Document(id=4),
            Document(id=5),
            Document(id=6),
            Document(id=7),
        ]
    )

    cache.delete(docs)
    assert cache.ids_count == 0
    assert cache.hashes_count == 0


def test_default_config(tmpdir):
    shutil.rmtree(os.path.join(cur_dir, '..', 'cache'), ignore_errors=True)
    docs = DocumentArray(
        [
            Document(id=1, content='üêØ'),
            Document(id=2, content='üêØ'),
            Document(id=3, content='üêª'),
        ]
    )

    f = Flow(return_results=True).add(uses=default_config)

    with f:
        response = f.post(on='/index', inputs=docs, return_results=True)

        assert (
            len(response[0].data.docs) == 2
        )  # the duplicated Document is removed from the request
        assert set([doc.id for doc in response[0].data.docs]) == set(['1', '3'])

    docs_to_update = DocumentArray([Document(id=2, content='üêº')])

    with f:
        response = f.post(on='/update', inputs=docs_to_update, return_results=True)
        assert (
            len(response[0].data.docs) == 1
        )  # the Document with `id=2` is no longer duplicated.

    with f:
        response = f.post(on='/index', inputs=docs[-1], return_results=True)
        assert len(response[0].data.docs) == 0  # the Document has been cached
        f.post(on='/delete', inputs=docs[-1])
        response = f.post(on='/index', inputs=docs[-1], return_results=True)
        assert (
            len(response[0].data.docs) == 1
        )  # the Document is cached again after the deletion
